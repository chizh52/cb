{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.0.cloudera2\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.4.3 (default, Nov 17 2016 01:08:31)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "exec(open(os.path.join(os.environ['SPARK_HOME'], 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataPath = \"./\"\n",
    "graphPath = dataPath + \"trainGraph/\"\n",
    "usersToPredictPath = dataPath + \"prediction.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"csv\").option(\"delimiter\", \"\\t\")\\\n",
    "    .load(graphPath).withColumnRenamed(\"_c0\", \"user\").withColumnRenamed(\"_c1\", \"friendsString\")\\\n",
    "    .repartition(50)\\\n",
    "    .write.mode(\"overwrite\")\\\n",
    "    .parquet(dataPath + \"trainGraphRepartitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import abs, col, explode, collect_list, sort_array, size, split, concat_ws, lit\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def cutStartEndBrackets(s):\n",
    "    return s[2:-2]\n",
    "\n",
    "cutStartEndBracketsUDF = udf(cutStartEndBrackets, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = spark.read.parquet(dataPath + \"trainGraphRepartitioned\") \n",
    "\n",
    "userFriend = data.select(col(\"user\"), split(cutStartEndBracketsUDF(col(\"friendsString\")), \"\\),\\(\").alias(\"friendsMasks\"))\\\n",
    "    .withColumn(\"friendMask\", explode('friendsMasks'))\\\n",
    "    .withColumn(\"friend\", split(col(\"friendMask\"), \",\")[0])\\\n",
    "    .select(col(\"user\").cast(\"integer\"), col(\"friend\").cast(\"integer\"))\n",
    "\n",
    "usersWithCommonFriend = userFriend\\\n",
    "    .groupBy(\"friend\")\\\n",
    "    .agg(collect_list(\"user\").alias(\"usersWithCommonFriend\"))\\\n",
    "    .where(size(col(\"usersWithCommonFriend\")) >= 2) \\\n",
    "    .select(col(\"usersWithCommonFriend\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def pairsWithCommonFriend(usersWithCommonFriend):\n",
    "    pairs = []\n",
    "    for user1Index in range(0, len(usersWithCommonFriend)):\n",
    "         for user2Index in range(0, len(usersWithCommonFriend)):\n",
    "                if user1Index != user2Index:\n",
    "                    pairs.append((usersWithCommonFriend[user1Index], usersWithCommonFriend[user2Index]))\n",
    "    return pairs\n",
    "\n",
    "schema = ArrayType(StructType([\n",
    "    StructField(\"user1\", IntegerType(), False),\n",
    "    StructField(\"user2\", IntegerType(), False)\n",
    "]))\n",
    "         \n",
    "pairsWithCommonFriendUdf = udf(pairsWithCommonFriend, schema)\n",
    "\n",
    "pairsPath = dataPath + \"pairs\"\n",
    "\n",
    "commonFriendsCounts = usersWithCommonFriend\\\n",
    "        .select(pairsWithCommonFriendUdf(\"usersWithCommonFriend\").alias(\"pairsWithCommonFriend\"))\\\n",
    "        .where(size(col(\"pairsWithCommonFriend\")) > 0)\\\n",
    "        .write.mode(\"overwrite\").parquet(pairsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(pairsPath)\\\n",
    "    .withColumn(\"pairWithCommonFriend\", explode(\"pairsWithCommonFriend\"))\\\n",
    "    .drop(col(\"pairsWithCommonFriend\"))\\\n",
    "    .groupBy(col(\"pairWithCommonFriend\"))\\\n",
    "    .count()\\\n",
    "    .write.parquet(dataPath + \"pairsCount\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
