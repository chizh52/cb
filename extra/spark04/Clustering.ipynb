{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.2\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--conf spark.sql.catalogImplementation=in-memory pyspark-shell'\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://master.cluster-lab.com:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f406386b208>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kmeans](pics/kmeans.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kmeans_algo](pics/kmeans_algo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"comment_text\", StringType()),\n",
    "    StructField(\"toxic\", IntegerType()),\n",
    "    StructField(\"severe_toxic\", IntegerType()),\n",
    "    StructField(\"obscene\", IntegerType()),\n",
    "    StructField(\"threat\", IntegerType()),\n",
    "    StructField(\"insult\", IntegerType()),\n",
    "    StructField(\"identity_hate\", IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = spark.read.csv(\"/user/pavel.klemenkov/lectures/lecture03/data/train.csv\", schema=schema, header=True, multiLine=True, escape='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.repartition(4).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, comment_text: string, toxic: int, severe_toxic: int, obscene: int, threat: int, insult: int, identity_hate: int]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"comment_text\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = StopWordsRemover.loadDefaultStopWords(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words_filtered\", stopWords=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(inputCol=swr.getOutputCol(), outputCol=\"word_vector\", vocabSize=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    swr,\n",
    "    count_vectorizer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_model = preprocessing.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset = preprocessing_model.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|word_vector                                                                                                                            |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(200,[0,1,5,53,55,68,101,115,129],[2.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0])                                                               |\n",
      "|(200,[0,1,70,78,101,179],[6.0,1.0,1.0,1.0,1.0,1.0])                                                                                    |\n",
      "|(200,[0,2,3,4,9,13,18,22,43,44,49,53,70,105,113,128,148,156],[1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|(200,[0,11,49,128,131],[2.0,1.0,2.0,1.0,1.0])                                                                                          |\n",
      "|(200,[7,8,22,28,31,38,42,44,46,50,72,76,102,132,160],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0])                    |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessed_dataset.select([\"word_vector\"]).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, comment_text: string, toxic: int, severe_toxic: int, obscene: int, threat: int, insult: int, identity_hate: int]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(featuresCol=\"word_vector\", k=7, seed=5757)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model = kmeans.fit(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = kmeans_model.transform(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=2),\n",
       " Row(toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=0),\n",
       " Row(toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=2),\n",
       " Row(toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=2),\n",
       " Row(toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=2),\n",
       " Row(toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=2),\n",
       " Row(toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=2),\n",
       " Row(toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=2),\n",
       " Row(toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=2),\n",
       " Row(toxic=1, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, prediction=2)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering[clustering.columns[2:8] + [\"prediction\"]].take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ClusteringEvaluator(featuresCol=\"word_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37089016703400873"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " comment_text | ....... \n",
      "now that i think about it, luna means moon and satin means devil!\n",
      "so that means you are the moon devil who else would be the moon devil\n",
      "but the person who disabled my ACCOUNT!!! ( if trevor is reading this \n",
      "then i want to say that i love that snowman, i tred to make one but it\n",
      "didnt work(scroll down))\n",
      "\n",
      "further\n",
      "\n",
      "even further\n",
      "\n",
      "almost there....\n",
      "\n",
      " __\n",
      "(**)\n",
      "( .)\n",
      "(. )\n",
      "                          O   \n",
      "                         O O                  \n",
      "                        O   O                        \n",
      "                       O     O                            \n",
      "                      O       O                      \n",
      "                     O         O                       \n",
      "                    O           O                     \n",
      "                   O             O               \n",
      "                  O               O                                                             \n",
      "                 O                 O          \n",
      "                O                   O       \n",
      "               O                     O         \n",
      "              O                       O         \n",
      "             O                         O\n",
      "            OOOOOOOOOOOOOOOOOOOOOOOOOOOO                               \n",
      "           O  O                      O  O                             \n",
      "          O    O                    O    O                    \n",
      "         O      O                  O      O                        \n",
      "        O        O                O        O                         \n",
      "       O          O              O          O               \n",
      "      O            O            O            O              \n",
      "     O              O          O              O     \n",
      "    O                O        O                O                     \n",
      "   O                  O      O                  O                                    \n",
      "  O                    O    O                    O                                               \n",
      " O                      O  O                      O                          \n",
      "O                        OO                        O \n",
      "OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO  SKILLS! IT TOOK 20 MINUTES TOO MAKE                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      "-RECORD 1----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " comment_text | \"\n",
      "\n",
      "As near as I can tell, wikipedia has no article on \"\"cycle structure\"\". The cycle structure of an element g of a permutation group is the multi-set of sizes of orbits of the cyclic group generated by g. This is related to the cycle decomposition of g when g only has finite orbits, which takes orbit representatives a1, a2, ... and specifies g by (a1, a1^g, a1^(g^2), ... )(a2, a2^g, ...) where each tuple has no duplicate entries. This is also called \"\"cycle notation\"\", and the \"\"cycle structure\"\" is the list of cycle sizes.\n",
      "For GAP, you can handle much larger groups using its character table library.  The following gives all the sizes, orders, and power relations for the O'Nan group. To check that 8a and 8b are not power conjugate, use the last commands.\n",
      " gap> LoadPackage(\"\"ctbllib\"\");;\n",
      " # Get the O'Nan group as a character table\n",
      " gap> ct = CharacterTable(\"\"ON\"\");;\n",
      " gap> OrdersClassRepresentatives(ct);\n",
      " gap> SizesConjugacyClasses(ct);\n",
      " # See which classes are 13th powers of each other\n",
      " gap> List( Filtered(\n",
      " > Cycles(PermList(PowerMap(ct,13)), [1..NrConjugacyClasses(ct)]),\n",
      " > x->Length(x)>1), x->ClassNames(ct){x});\n",
      " [ [ \"\"15a\"\", \"\"15b\"\" ], [ \"\"16a\"\", \"\"16b\"\" ], [ \"\"16c\"\", \"\"16d\"\" ], [ \"\"19a\"\", \"\"19c\"\", \"\"19b\"\" ],\n",
      "   [ \"\"20a\"\", \"\"20b\"\" ], [ \"\"28a\"\", \"\"28b\"\" ], [ \"\"31a\"\", \"\"31b\"\" ] ]\n",
      " gap> PrintArray(TransposedMat([ClassNames(ct), \n",
      " > OrdersClassRepresentatives(ct),\n",
      " > SizesConjugacyClasses(ct),\n",
      " > List(SizesConjugacyClasses(ct),StringPP)]));\n",
      " [[  1a,  1,           1,                       1 ],                                                             \n",
      "  [  2a,  2,     2857239,        3^2*7^2*11*19*31 ],                                                             \n",
      "  [  3a,  3,   142227008,        2^6*7^3*11*19*31 ],                                                             \n",
      "  [  4a,  4,     5714478,      2*3^2*7^2*11*19*31 ],                                                             \n",
      "  [  4b,  4,  1800060570,    2*3^4*5*7^3*11*19*31 ],                                                             \n",
      "  [  5a,  5,  2560086144,    2^7*3^2*7^3*11*19*31 ],                                                             \n",
      "  [  6a,  6,  6400215360,  2^6*3^2*5*7^3*11*19*31 ],                                                             \n",
      "  [  7a,  7,   335871360,      2^7*3^4*5*11*19*31 ],                                                             \n",
      "  [  7b,  7,  9404398080,    2^9*3^4*5*7*11*19*31 ],                                                             \n",
      "  [  8a,  8, 14400484560,  2^4*3^4*5*7^3*11*19*31 ],                                                             \n",
      "  [  8b,  8, 14400484560,  2^4*3^4*5*7^3*11*19*31 ],                                                             \n",
      "  [ 10a, 10, 23040775296,    2^7*3^4*7^3*11*19*31 ],                                                             \n",
      "  [ 11a, 11, 41892318720,     2^9*3^4*5*7^3*19*31 ],                                                             \n",
      "  [ 12a, 12, 12800430720,  2^7*3^2*5*7^3*11*19*31 ],                                                             \n",
      "  [ 14a, 14, 16457696640,  2^7*3^4*5*7^2*11*19*31 ],                                                             \n",
      "  [ 15a, 15, 10240344576,    2^9*3^2*7^3*11*19*31 ],                                                             \n",
      "  [ 15b, 15, 10240344576,    2^9*3^2*7^3*11*19*31 ],                                                             \n",
      "  [ 16a, 16, 28800969120,  2^5*3^4*5*7^3*11*19*31 ],                                                             \n",
      "  [ 16b, 16, 28800969120,  2^5*3^4*5*7^3*11*19*31 ],                                                             \n",
      "  [ 16c, 16, 28800969120,  2^5*3^4*5*7^3*11*19*31 ],                                                             \n",
      "  [ 16d, 16, 28800969120,  2^5*3^4*5*7^3*11*19*31 ],                                                             \n",
      "  [ 19a, 19, 24253447680,     2^9*3^4*5*7^3*11*31 ],                                                             \n",
      "  [ 19b, 19, 24253447680,     2^9*3^4*5*7^3*11*31 ],                                                             \n",
      "  [ 19c, 19, 24253447680,     2^9*3^4*5*7^3*11*31 ],                                                             \n",
      "  [ 20a, 20, 23040775296,    2^7*3^4*7^3*11*19*31 ],                                                             \n",
      "  [ 20b, 20, 23040775296,    2^7*3^4*7^3*11*19*31 ],                                                             \n",
      "  [ 28a, 28, 16457696640,  2^7*3^4*5*7^2*11*19*31 ],                                                             \n",
      "  [ 28b, 28, 16457696640,  2^7*3^4*5*7^2*11*19*31 ],                                                             \n",
      "  [ 31a, 31, 14865016320,     2^9*3^4*                                                                                                      \n",
      "-RECORD 2----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " comment_text | ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     ANAL RAPE     \n",
      "AN \n",
      "\n"
     ]
    }
   ],
   "source": [
    "clustering.filter(clustering.prediction == 1)[[\"comment_text\"]].show(5, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(featuresCol=\"word_vector\", k=2, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_model = kmeans.fit(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = kmeans_model.transform(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9994404484935104"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([3.0109565 , 0.51180268, 0.24449041, 0.17984831, 0.17243325,\n",
       "        0.16500564, 0.15431867, 0.14628306, 0.14062304, 0.14034725,\n",
       "        0.11886674, 0.11762567, 0.10255735, 0.10247587, 0.09564999,\n",
       "        0.09480381, 0.0941394 , 0.09082362, 0.08216748, 0.08130876,\n",
       "        0.07923405, 0.07139275, 0.07050896, 0.06879153, 0.06503071,\n",
       "        0.06314404, 0.06249216, 0.06222264, 0.06039865, 0.05926413,\n",
       "        0.05809828, 0.05765325, 0.05692616, 0.0554093 , 0.05474489,\n",
       "        0.0546446 , 0.05430613, 0.05344114, 0.05312147, 0.05280807,\n",
       "        0.05084618, 0.05274539, 0.05250094, 0.05196816, 0.0514103 ,\n",
       "        0.05116585, 0.05008148, 0.04925411, 0.04914128, 0.04869625,\n",
       "        0.04811959, 0.0476871 , 0.04764949, 0.04752413, 0.04740504,\n",
       "        0.04605115, 0.04578789, 0.04533659, 0.04526138, 0.0452175 ,\n",
       "        0.0446095 , 0.04420835, 0.04403284, 0.04393882, 0.04333709,\n",
       "        0.04328695, 0.0428858 , 0.04227153, 0.04097405, 0.04091764,\n",
       "        0.04075467, 0.04058543, 0.04028457, 0.04006519, 0.03988341,\n",
       "        0.03876144, 0.03849191, 0.03846684, 0.0379654 , 0.03737621,\n",
       "        0.03730099, 0.03717563, 0.03703147, 0.03688103, 0.03677448,\n",
       "        0.03660524, 0.03647988, 0.03649868, 0.03646108, 0.03640466,\n",
       "        0.03622289, 0.03616021, 0.03586561, 0.03553341, 0.03550207,\n",
       "        0.0354958 , 0.0350445 , 0.03480632, 0.03406042, 0.03391626,\n",
       "        0.03329573, 0.03284443, 0.03223016, 0.03187288, 0.03176006,\n",
       "        0.0314592 , 0.03125235, 0.03120221, 0.03099536, 0.03095149,\n",
       "        0.03080105, 0.0307321 , 0.03059421, 0.0305378 , 0.03037483,\n",
       "        0.03031215, 0.0302432 , 0.03018679, 0.03013664, 0.02984205,\n",
       "        0.02966027, 0.02955372, 0.0294973 , 0.02942209, 0.02926539,\n",
       "        0.02927792, 0.02915883, 0.02911496, 0.02892065, 0.02891438,\n",
       "        0.02858217, 0.02845681, 0.02816848, 0.02811834, 0.02808073,\n",
       "        0.02802432, 0.02788016, 0.02787389, 0.02778614, 0.02776733,\n",
       "        0.02762943, 0.02745393, 0.02725962, 0.02718441, 0.02696502,\n",
       "        0.02667043, 0.02657641, 0.02651373, 0.0264009 , 0.02620659,\n",
       "        0.02618779, 0.02613765, 0.02603736, 0.02517237, 0.02570515,\n",
       "        0.02554218, 0.02543563, 0.02542309, 0.02539175, 0.02519117,\n",
       "        0.02512849, 0.02494045, 0.024696  , 0.02458944, 0.02430112,\n",
       "        0.02409427, 0.02369939, 0.02365551, 0.02356149, 0.02354269,\n",
       "        0.02324182, 0.02322928, 0.02293469, 0.02290962, 0.02270277,\n",
       "        0.02265889, 0.02262129, 0.02247712, 0.02244578, 0.02243325,\n",
       "        0.02232669, 0.02228908, 0.02226401, 0.0222076 , 0.02202582,\n",
       "        0.02201329, 0.02196941, 0.02184405, 0.02170616, 0.02165601,\n",
       "        0.02156826, 0.02152438, 0.02145543, 0.02139902, 0.02129873,\n",
       "        0.021305  , 0.02119218, 0.02113577, 0.02109816, 0.02105428]),\n",
       " array([6.00322581e+02, 5.48387097e-01, 3.22580645e-01, 1.09677419e+00,\n",
       "        6.45161290e-02, 3.87096774e-01, 4.51612903e-01, 1.03225806e+00,\n",
       "        9.67741935e-02, 3.22580645e-02, 1.93548387e-01, 1.29032258e-01,\n",
       "        2.58064516e-01, 5.16129032e-01, 9.67741935e-02, 2.25806452e-01,\n",
       "        2.25806452e-01, 1.29032258e-01, 5.16129032e-01, 1.29032258e-01,\n",
       "        2.25806452e-01, 0.00000000e+00, 1.61290323e-01, 1.93548387e-01,\n",
       "        9.67741935e-02, 3.22580645e-02, 1.93548387e-01, 1.29032258e-01,\n",
       "        0.00000000e+00, 1.93548387e-01, 9.67741935e-02, 6.45161290e-02,\n",
       "        3.22580645e-02, 6.45161290e-02, 9.67741935e-02, 9.67741935e-02,\n",
       "        3.22580645e-01, 3.22580645e-02, 3.22580645e-02, 9.67741935e-02,\n",
       "        1.01290323e+01, 1.29032258e-01, 1.29032258e-01, 6.45161290e-02,\n",
       "        9.67741935e-02, 6.45161290e-02, 6.45161290e-02, 2.90322581e-01,\n",
       "        2.25806452e-01, 1.29032258e-01, 1.29032258e-01, 6.45161290e-02,\n",
       "        3.22580645e-02, 1.61290323e-01, 9.67741935e-02, 2.90322581e-01,\n",
       "        1.61290323e-01, 3.22580645e-02, 1.93548387e-01, 3.22580645e-02,\n",
       "        9.67741935e-02, 9.67741935e-02, 6.45161290e-02, 6.45161290e-02,\n",
       "        3.22580645e-02, 6.45161290e-02, 6.45161290e-02, 6.45161290e-02,\n",
       "        6.45161290e-02, 3.22580645e-02, 3.22580645e-02, 3.22580645e-02,\n",
       "        3.22580645e-02, 6.45161290e-02, 3.22580645e-02, 6.45161290e-02,\n",
       "        0.00000000e+00, 3.22580645e-02, 6.45161290e-02, 6.45161290e-02,\n",
       "        6.45161290e-02, 9.67741935e-02, 6.45161290e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 9.67741935e-02, 1.29032258e-01, 0.00000000e+00,\n",
       "        1.29032258e-01, 0.00000000e+00, 0.00000000e+00, 1.29032258e-01,\n",
       "        3.22580645e-02, 6.45161290e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.61290323e-01, 1.29032258e-01, 3.22580645e-02, 1.29032258e-01,\n",
       "        0.00000000e+00, 3.22580645e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        9.67741935e-02, 0.00000000e+00, 6.45161290e-02, 1.29032258e-01,\n",
       "        0.00000000e+00, 9.67741935e-02, 3.22580645e-02, 3.22580645e-02,\n",
       "        0.00000000e+00, 6.45161290e-02, 3.22580645e-02, 1.29032258e-01,\n",
       "        0.00000000e+00, 9.67741935e-02, 3.22580645e-02, 3.22580645e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.22580645e-02, 0.00000000e+00,\n",
       "        1.29032258e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.29032258e-01, 3.22580645e-02, 3.22580645e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 6.45161290e-02, 9.67741935e-02, 9.67741935e-02,\n",
       "        0.00000000e+00, 3.22580645e-02, 9.67741935e-02, 0.00000000e+00,\n",
       "        2.25806452e-01, 3.22580645e-02, 1.61290323e-01, 6.45161290e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.22580645e-02, 0.00000000e+00,\n",
       "        3.22580645e-02, 0.00000000e+00, 0.00000000e+00, 3.22580645e-02,\n",
       "        3.22580645e-02, 4.03225806e+00, 0.00000000e+00, 3.22580645e-02,\n",
       "        9.67741935e-02, 3.22580645e-02, 3.22580645e-02, 3.87096774e-01,\n",
       "        0.00000000e+00, 3.22580645e-02, 3.22580645e-02, 0.00000000e+00,\n",
       "        1.29032258e-01, 3.22580645e-02, 6.45161290e-02, 3.22580645e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.22580645e-02,\n",
       "        0.00000000e+00, 3.22580645e-02, 0.00000000e+00, 2.25806452e-01,\n",
       "        0.00000000e+00, 6.45161290e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.22580645e-02, 3.22580645e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.45161290e-02, 0.00000000e+00,\n",
       "        3.22580645e-02, 0.00000000e+00, 3.22580645e-02, 0.00000000e+00,\n",
       "        6.45161290e-02, 3.22580645e-02, 1.93548387e-01, 0.00000000e+00,\n",
       "        3.22580645e-02, 0.00000000e+00, 0.00000000e+00, 3.22580645e-02])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_model.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0109564999373197"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(kmeans_model.clusterCenters()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  41,  42,  43,  44,  45,  40,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  87,  86,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 125, 124, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 154, 155, 156,\n",
       "       157, 158, 159, 153, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 195,\n",
       "       194, 196, 197, 198, 199])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(-kmeans_model.clusterCenters()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  40, 153,   3,   7,   1,  18,  13,   6,   5, 159,   2,  36,\n",
       "        55,  47,  12,  15,  16, 175, 140,  20,  48, 194,  58,  26,  29,\n",
       "        23,  10,  22, 142,  96,  53,  56,  49,  50, 164, 124,  86,  88,\n",
       "        91,  97, 107, 115, 128,  99,  17,  42,  41,  11,  27,  19,  24,\n",
       "        85,   8,  30, 156,  14, 104,  61,  60,  44,  34, 117,  39, 134,\n",
       "       109, 135,  81, 138,  35,  54, 113, 143, 106,  31,  33,  82, 166,\n",
       "       133,  93,  45,  79,  63,  62,  80,  67,  68, 186, 192,  66, 177,\n",
       "         4,  51,  43,  46,  73,  75,  78,  65, 137, 193, 141, 190, 130,\n",
       "       129, 196, 151, 146, 148, 152, 155, 157, 158, 181, 180, 161, 162,\n",
       "         9, 165, 173, 171, 188, 167, 122, 199,  74,  98,  71,  70,  32,\n",
       "        69, 110, 111,  92, 101, 114,  64,  25,  59, 118,  57, 119,  37,\n",
       "        38,  52,  77,  72, 170, 172,  76, 174, 169, 168, 184, 178, 179,\n",
       "       182, 183, 185, 187, 189, 191, 195, 197, 176,  83, 121,  87, 125,\n",
       "       126, 127,  21, 120, 131, 132, 116, 136, 112, 139, 108, 105, 144,\n",
       "        84, 145, 147, 102, 149, 150, 100, 198, 154,  28,  95,  94, 160,\n",
       "       123,  89, 163, 103,  90])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(-kmeans_model.clusterCenters()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '\"',\n",
       " 'article',\n",
       " 'page',\n",
       " 'please',\n",
       " 'like',\n",
       " 'one',\n",
       " '-',\n",
       " 'wikipedia',\n",
       " 'talk',\n",
       " 'think',\n",
       " 'see',\n",
       " 'also',\n",
       " 'know',\n",
       " 'may',\n",
       " 'edit',\n",
       " 'people',\n",
       " 'use',\n",
       " 'get',\n",
       " 'even',\n",
       " 'make',\n",
       " 'articles',\n",
       " 'good',\n",
       " 'want',\n",
       " 'time',\n",
       " 'it.',\n",
       " 'need',\n",
       " 'new',\n",
       " 'thank',\n",
       " 'go',\n",
       " 'first',\n",
       " 'information',\n",
       " 'many',\n",
       " 'made',\n",
       " 'find',\n",
       " 'page.',\n",
       " 'name',\n",
       " 'really',\n",
       " 'thanks',\n",
       " 'say',\n",
       " 'fuck',\n",
       " 'much',\n",
       " 'used',\n",
       " 'since',\n",
       " 'article.',\n",
       " 'user',\n",
       " 'add',\n",
       " 'way',\n",
       " 'take',\n",
       " 'help',\n",
       " 'sources',\n",
       " 'look',\n",
       " 'someone',\n",
       " 'still',\n",
       " 'read',\n",
       " 'section',\n",
       " 'pages',\n",
       " 'going',\n",
       " 'two',\n",
       " 'deletion',\n",
       " 'you.',\n",
       " 'source',\n",
       " 'edits',\n",
       " 'without',\n",
       " 'discussion',\n",
       " 'well',\n",
       " 'editing',\n",
       " 'wikipedia.',\n",
       " 'point',\n",
       " 'deleted',\n",
       " 'back',\n",
       " 'might',\n",
       " 'work',\n",
       " 'something',\n",
       " 'image',\n",
       " 'another',\n",
       " 'added',\n",
       " 'never',\n",
       " 'put',\n",
       " 'link',\n",
       " 'seems',\n",
       " 'stop',\n",
       " ',',\n",
       " 'blocked',\n",
       " 'feel',\n",
       " '.',\n",
       " 'list',\n",
       " 'block',\n",
       " 'right',\n",
       " 'said',\n",
       " '(utc)',\n",
       " 'using',\n",
       " 'ask',\n",
       " 'personal',\n",
       " 'fact',\n",
       " 'sure',\n",
       " 'article,',\n",
       " 'believe',\n",
       " 'hope',\n",
       " 'page,',\n",
       " 'note',\n",
       " 'actually',\n",
       " 'editors',\n",
       " 'keep',\n",
       " 'place',\n",
       " '',\n",
       " 'remove',\n",
       " 'better',\n",
       " 'done',\n",
       " 'part',\n",
       " 'free',\n",
       " 'trying',\n",
       " 'reason',\n",
       " 'comment',\n",
       " 'it,',\n",
       " 'little',\n",
       " 'must',\n",
       " 'links',\n",
       " 'content',\n",
       " 'best',\n",
       " 'history',\n",
       " 'speedy',\n",
       " 'already',\n",
       " 'anything',\n",
       " 'give',\n",
       " 'nothing',\n",
       " 'copyright',\n",
       " 'us',\n",
       " 'let',\n",
       " 'removed',\n",
       " 'things',\n",
       " 'hi',\n",
       " 'however,',\n",
       " 'comments',\n",
       " 'last',\n",
       " 'wiki',\n",
       " 'making',\n",
       " 'rather',\n",
       " 'change',\n",
       " 'me.',\n",
       " 'come',\n",
       " 'welcome',\n",
       " 'person',\n",
       " 'anyone',\n",
       " 'question',\n",
       " 'try',\n",
       " 'understand',\n",
       " 'different',\n",
       " 'here.',\n",
       " 'got',\n",
       " 'leave',\n",
       " 'continue',\n",
       " 'every',\n",
       " '|',\n",
       " 'simply',\n",
       " 'long',\n",
       " 'found',\n",
       " 'probably',\n",
       " 'reliable',\n",
       " '(talk)',\n",
       " 'adding',\n",
       " 'you,',\n",
       " 'original',\n",
       " 'agree',\n",
       " ')',\n",
       " 'check',\n",
       " 'case',\n",
       " 'delete',\n",
       " 'tag',\n",
       " 'great',\n",
       " 'fair',\n",
       " 'subject',\n",
       " 'thing',\n",
       " 'mean',\n",
       " 'seem',\n",
       " 'u',\n",
       " 'reference',\n",
       " 'least',\n",
       " 'says',\n",
       " 'needs',\n",
       " 'problem',\n",
       " 'ip',\n",
       " 'write',\n",
       " 'show',\n",
       " 'given',\n",
       " 'editor',\n",
       " 'called',\n",
       " 'clearly',\n",
       " 'word',\n",
       " 'thought',\n",
       " 'quite',\n",
       " 'policy',\n",
       " 'tell',\n",
       " 'lot',\n",
       " 'text',\n",
       " 'far',\n",
       " 'always',\n",
       " 'also,',\n",
       " '',\n",
       " 'whether']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_model.stages[2].vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fuck\n",
      "|\n",
      "page\n",
      "-\n",
      "\"\n",
      "get\n",
      "know\n",
      "one\n",
      "like\n",
      "(talk)\n",
      "article\n",
      "name\n",
      "section\n",
      "way\n",
      "also\n",
      "edit\n",
      "people\n",
      "u\n",
      "come\n",
      "make\n",
      "take\n",
      "text\n",
      "two\n",
      "need\n",
      "go\n",
      "want\n",
      "think\n",
      "good\n",
      "person\n",
      "article,\n",
      "still\n",
      "pages\n",
      "help\n",
      "sources\n",
      ")\n",
      "give\n",
      "list\n",
      "right\n",
      "using\n"
     ]
    }
   ],
   "source": [
    "for i in np.argsort(-kmeans_model.clusterCenters()[1])[:40]:\n",
    "    print(preprocessing_model.stages[2].vocabulary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"\n",
      "article\n",
      "page\n",
      "please\n",
      "like\n",
      "one\n",
      "-\n",
      "wikipedia\n",
      "talk\n",
      "think\n",
      "see\n",
      "also\n",
      "know\n",
      "may\n",
      "edit\n",
      "people\n",
      "use\n",
      "get\n",
      "even\n",
      "make\n",
      "articles\n",
      "good\n",
      "want\n",
      "time\n",
      "it.\n",
      "need\n",
      "new\n",
      "thank\n",
      "go\n",
      "first\n",
      "information\n",
      "many\n",
      "made\n",
      "find\n",
      "page.\n",
      "name\n",
      "really\n",
      "thanks\n",
      "say\n"
     ]
    }
   ],
   "source": [
    "for i in np.argsort(-kmeans_model.clusterCenters()[0])[:40]:\n",
    "    print(preprocessing_model.stages[2].vocabulary[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The curse of dimensionality\n",
    "![curse](pics/dimensionality_vs_performance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is that?\n",
    "![curse](pics/curseofdimensionality.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![curse](pics/lda.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![curse](pics/dirichlet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(featuresCol=\"word_vector\", seed=5757, k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = lda.fit(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda_model.transform(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='6fdb7b6734f8bf40', comment_text='\"\\n\\n\"\"Katara\"\"\\nI\\'ve removed the section entirely. I don\\'t care if you like to pretend that Katara and Zuko are meant for each other. It\\'s still not case, and there has been no indication whatsoever. Thus, there\\'s little point to actually have the section and exempt Toph and Sokka beyond the insane delusions of shippers.  \"', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, words=['\"', '', '\"\"katara\"\"', \"i've\", 'removed', 'the', 'section', 'entirely.', 'i', \"don't\", 'care', 'if', 'you', 'like', 'to', 'pretend', 'that', 'katara', 'and', 'zuko', 'are', 'meant', 'for', 'each', 'other.', \"it's\", 'still', 'not', 'case,', 'and', 'there', 'has', 'been', 'no', 'indication', 'whatsoever.', 'thus,', \"there's\", 'little', 'point', 'to', 'actually', 'have', 'the', 'section', 'and', 'exempt', 'toph', 'and', 'sokka', 'beyond', 'the', 'insane', 'delusions', 'of', 'shippers.', '', '\"'], words_filtered=['\"', '', '\"\"katara\"\"', 'removed', 'section', 'entirely.', 'care', 'like', 'pretend', 'katara', 'zuko', 'meant', 'other.', 'still', 'case,', 'indication', 'whatsoever.', 'thus,', 'little', 'point', 'actually', 'section', 'exempt', 'toph', 'sokka', 'beyond', 'insane', 'delusions', 'shippers.', '', '\"'], word_vector=SparseVector(200, {0: 2.0, 1: 2.0, 5: 1.0, 53: 1.0, 55: 2.0, 68: 1.0, 101: 1.0, 115: 1.0, 129: 1.0}), topicDistribution=DenseVector([0.0118, 0.0116, 0.011, 0.3919, 0.5622, 0.0116])),\n",
       " Row(id='26e1b63617df36b1', comment_text='\"\\n\\n charlie wilson \\n\\ni didnt notice the music genres that were reverted. However my intention was to revert his alias that you deleted.  His alias a.k.a is actually \"\"Uncle Charlie\"\" and needs to be put back and shouldn\\'t  have been removed.\"', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, words=['\"', '', '', 'charlie', 'wilson', '', '', 'i', 'didnt', 'notice', 'the', 'music', 'genres', 'that', 'were', 'reverted.', 'however', 'my', 'intention', 'was', 'to', 'revert', 'his', 'alias', 'that', 'you', 'deleted.', '', 'his', 'alias', 'a.k.a', 'is', 'actually', '\"\"uncle', 'charlie\"\"', 'and', 'needs', 'to', 'be', 'put', 'back', 'and', \"shouldn't\", '', 'have', 'been', 'removed.\"'], words_filtered=['\"', '', '', 'charlie', 'wilson', '', '', 'didnt', 'notice', 'music', 'genres', 'reverted.', 'however', 'intention', 'revert', 'alias', 'deleted.', '', 'alias', 'a.k.a', 'actually', '\"\"uncle', 'charlie\"\"', 'needs', 'put', 'back', '', 'removed.\"'], word_vector=SparseVector(200, {0: 6.0, 1: 1.0, 70: 1.0, 78: 1.0, 101: 1.0, 179: 1.0}), topicDistribution=DenseVector([0.0128, 0.0125, 0.0118, 0.0139, 0.9363, 0.0125])),\n",
       " Row(id='85e4f353ca4b2bde', comment_text=\"Arthur Rose Eldred GA Sweeps: On Hold\\nI have reviewed Arthur Rose Eldred for GA Sweeps to determine if it still qualifies as a Good Article. In reviewing the article I have found several issues, which I have detailed here. Since you are a main contributor of the article (determined based on this tool), I figured you would be interested in contributing to further improve the article. Please comment there to help the article maintain its GA status. If you have any questions, let me know on my talk page and I'll get back to you as soon as I can. Happy editing!  (talk  contrib)\", toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, words=['arthur', 'rose', 'eldred', 'ga', 'sweeps:', 'on', 'hold', 'i', 'have', 'reviewed', 'arthur', 'rose', 'eldred', 'for', 'ga', 'sweeps', 'to', 'determine', 'if', 'it', 'still', 'qualifies', 'as', 'a', 'good', 'article.', 'in', 'reviewing', 'the', 'article', 'i', 'have', 'found', 'several', 'issues,', 'which', 'i', 'have', 'detailed', 'here.', 'since', 'you', 'are', 'a', 'main', 'contributor', 'of', 'the', 'article', '(determined', 'based', 'on', 'this', 'tool),', 'i', 'figured', 'you', 'would', 'be', 'interested', 'in', 'contributing', 'to', 'further', 'improve', 'the', 'article.', 'please', 'comment', 'there', 'to', 'help', 'the', 'article', 'maintain', 'its', 'ga', 'status.', 'if', 'you', 'have', 'any', 'questions,', 'let', 'me', 'know', 'on', 'my', 'talk', 'page', 'and', \"i'll\", 'get', 'back', 'to', 'you', 'as', 'soon', 'as', 'i', 'can.', 'happy', 'editing!', '', '(talk', '', 'contrib)'], words_filtered=['arthur', 'rose', 'eldred', 'ga', 'sweeps:', 'hold', 'reviewed', 'arthur', 'rose', 'eldred', 'ga', 'sweeps', 'determine', 'still', 'qualifies', 'good', 'article.', 'reviewing', 'article', 'found', 'several', 'issues,', 'detailed', 'here.', 'since', 'main', 'contributor', 'article', '(determined', 'based', 'tool),', 'figured', 'interested', 'contributing', 'improve', 'article.', 'please', 'comment', 'help', 'article', 'maintain', 'ga', 'status.', 'questions,', 'let', 'know', 'talk', 'page', 'get', 'back', 'soon', 'can.', 'happy', 'editing!', '', '(talk', '', 'contrib)'], word_vector=SparseVector(200, {0: 1.0, 2: 3.0, 3: 1.0, 4: 1.0, 9: 1.0, 13: 1.0, 18: 1.0, 22: 1.0, 43: 1.0, 44: 2.0, 49: 1.0, 53: 1.0, 70: 1.0, 105: 1.0, 113: 1.0, 128: 1.0, 148: 1.0, 156: 1.0}), topicDistribution=DenseVector([0.5708, 0.4, 0.0065, 0.0076, 0.0082, 0.0068])),\n",
       " Row(id='9d2196265213dce8', comment_text='Re: Help translate from English (posted into catalan wikipedia) \\n\\nHi Xaris333. Let me see if I can help you...', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, words=['re:', 'help', 'translate', 'from', 'english', '(posted', 'into', 'catalan', 'wikipedia)', '', '', 'hi', 'xaris333.', 'let', 'me', 'see', 'if', 'i', 'can', 'help', 'you...'], words_filtered=['re:', 'help', 'translate', 'english', '(posted', 'catalan', 'wikipedia)', '', '', 'hi', 'xaris333.', 'let', 'see', 'help', 'you...'], word_vector=SparseVector(200, {0: 2.0, 11: 1.0, 49: 2.0, 128: 1.0, 131: 1.0}), topicDistribution=DenseVector([0.1318, 0.7878, 0.0178, 0.0208, 0.0229, 0.0189])),\n",
       " Row(id='fb7a63a8e287b2d1', comment_text='Sources for Gambia at the 2000 Summer Olympics (\\nHello, good work on Gambia at the 2000 Summer Olympics (, and thanks for the contribution. However, you forgot to add any references to the article. Keeping Wikipedia accurate and verifiable is very important, and there is currently a push to encourage editors to cite the sources they used when adding content. What websites, books, or other places did you learn the information that you added to Gambia at the 2000 Summer Olympics (? Would it be possible for you to mention them in the article? Thank you very much. -', toxic=0, severe_toxic=0, obscene=0, threat=0, insult=0, identity_hate=0, words=['sources', 'for', 'gambia', 'at', 'the', '2000', 'summer', 'olympics', '(', 'hello,', 'good', 'work', 'on', 'gambia', 'at', 'the', '2000', 'summer', 'olympics', '(,', 'and', 'thanks', 'for', 'the', 'contribution.', 'however,', 'you', 'forgot', 'to', 'add', 'any', 'references', 'to', 'the', 'article.', 'keeping', 'wikipedia', 'accurate', 'and', 'verifiable', 'is', 'very', 'important,', 'and', 'there', 'is', 'currently', 'a', 'push', 'to', 'encourage', 'editors', 'to', 'cite', 'the', 'sources', 'they', 'used', 'when', 'adding', 'content.', 'what', 'websites,', 'books,', 'or', 'other', 'places', 'did', 'you', 'learn', 'the', 'information', 'that', 'you', 'added', 'to', 'gambia', 'at', 'the', '2000', 'summer', 'olympics', '(?', 'would', 'it', 'be', 'possible', 'for', 'you', 'to', 'mention', 'them', 'in', 'the', 'article?', 'thank', 'you', 'very', 'much.', '-'], words_filtered=['sources', 'gambia', '2000', 'summer', 'olympics', '(', 'hello,', 'good', 'work', 'gambia', '2000', 'summer', 'olympics', '(,', 'thanks', 'contribution.', 'however,', 'forgot', 'add', 'references', 'article.', 'keeping', 'wikipedia', 'accurate', 'verifiable', 'important,', 'currently', 'push', 'encourage', 'editors', 'cite', 'sources', 'used', 'adding', 'content.', 'websites,', 'books,', 'places', 'learn', 'information', 'added', 'gambia', '2000', 'summer', 'olympics', '(?', 'possible', 'mention', 'article?', 'thank', 'much.', '-'], word_vector=SparseVector(200, {7: 1.0, 8: 1.0, 22: 1.0, 28: 1.0, 31: 1.0, 38: 1.0, 42: 1.0, 44: 1.0, 46: 1.0, 50: 2.0, 72: 1.0, 76: 1.0, 102: 1.0, 132: 1.0, 160: 1.0}), topicDistribution=DenseVector([0.4623, 0.0088, 0.0084, 0.0097, 0.0106, 0.5002]))]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.vocabSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(topic=0, termIndices=[7, 0, 2, 8, 1, 15, 3, 6, 131, 82], termWeights=[0.06021143367631872, 0.057629535517439316, 0.05682854781977034, 0.03824067058005817, 0.03352867303981374, 0.02674318119930803, 0.020962483900486363, 0.02014963535909523, 0.017994044037643337, 0.01603144346798472]),\n",
       " Row(topic=1, termIndices=[0, 9, 1, 3, 49, 27, 8, 105, 56, 45], termWeights=[0.20897159583709132, 0.04427843581569699, 0.04230095072263008, 0.036887525279393055, 0.017060804342550105, 0.016202617806118664, 0.015977531088498063, 0.015728332085796257, 0.015688595003576014, 0.015101081536063305]),\n",
       " Row(topic=2, termIndices=[0, 40, 2, 59, 3, 4, 14, 69, 121, 1], termWeights=[0.09088524558404357, 0.06881859611232774, 0.0572980668112808, 0.048720330500939484, 0.038817531502747846, 0.03774703589267476, 0.03561011380738992, 0.034367993715631734, 0.032703661779232465, 0.028210862997463452]),\n",
       " Row(topic=3, termIndices=[5, 16, 10, 13, 6, 19, 18, 12, 0, 32], termWeights=[0.03721827676655505, 0.028764065122983237, 0.026411171168077986, 0.025358058590830956, 0.02109285315988282, 0.020339308626399388, 0.02008221037517457, 0.016828536873260466, 0.01584250498331804, 0.013472728108535668]),\n",
       " Row(topic=4, termIndices=[0, 1, 2, 6, 7, 10, 5, 12, 11, 14], termWeights=[0.5195738649131304, 0.06840482607317379, 0.012992317573674143, 0.012368482346891165, 0.010059770360028282, 0.00745809532364809, 0.007181142337611559, 0.006435242497664184, 0.006037787531254037, 0.00557191994276756]),\n",
       " Row(topic=5, termIndices=[0, 4, 17, 1, 28, 74, 3, 126, 60, 38], termWeights=[0.171415185617938, 0.06721936577820162, 0.038962736971120855, 0.03734022888392622, 0.027353790001595383, 0.023686197061492796, 0.021894656073535027, 0.02002271033171132, 0.018716698885967376, 0.018401243045963186])]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.describeTopics(maxTermsPerTopic=10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "\n",
      "article\n",
      "wikipedia\n",
      "\"\n",
      "edit\n",
      "page\n",
      "one\n",
      "hi\n",
      ",\n"
     ]
    }
   ],
   "source": [
    "for i in [7, 0, 2, 8, 1, 15, 3, 6, 131, 82]:\n",
    "    print(preprocessing_model.stages[-1].vocabulary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "talk\n",
      "\"\n",
      "page\n",
      "help\n",
      "new\n",
      "wikipedia\n",
      "\n",
      "pages\n",
      "user\n"
     ]
    }
   ],
   "source": [
    "for i in [0, 9, 1, 3, 49, 27, 8, 105, 56, 45]:\n",
    "    print(preprocessing_model.stages[-1].vocabulary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fuck\n",
      "article\n",
      "deletion\n",
      "page\n",
      "please\n",
      "may\n",
      "deleted\n",
      "speedy\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "for i in [0, 40, 2, 59, 3, 4, 14, 69, 121, 1]:\n",
    "    print(preprocessing_model.stages[-1].vocabulary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "please\n",
      "use\n",
      "\"\n",
      "thank\n",
      "image\n",
      "page\n",
      "copyright\n",
      "you.\n",
      "thanks\n"
     ]
    }
   ],
   "source": [
    "for i in [0, 4, 17, 1, 28, 74, 3, 126, 60, 38]:\n",
    "    print(preprocessing_model.stages[-1].vocabulary[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering is a good dimensionality reduction technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, comment_text: string, toxic: int, severe_toxic: int, obscene: int, threat: int, insult: int, identity_hate: int, words: array<string>, words_filtered: array<string>, word_vector: vector, topicDistribution: vector]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = f.when(\n",
    "    (topics.toxic == 0) &\n",
    "    (topics.severe_toxic == 0) &\n",
    "    (topics.obscene == 0) &\n",
    "    (topics.threat == 0) &\n",
    "    (topics.insult == 0) &\n",
    "    (topics.identity_hate == 0),\n",
    "    0\n",
    ").otherwise(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = topics.withColumn(\"target\", target)[[\"id\", \"target\", \"topicDistribution\"]].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='6fdb7b6734f8bf40', target=0, topicDistribution=DenseVector([0.0118, 0.0116, 0.011, 0.3918, 0.5622, 0.0116])),\n",
       " Row(id='26e1b63617df36b1', target=0, topicDistribution=DenseVector([0.0128, 0.0125, 0.0118, 0.0139, 0.9363, 0.0125])),\n",
       " Row(id='85e4f353ca4b2bde', target=0, topicDistribution=DenseVector([0.5708, 0.4, 0.0065, 0.0076, 0.0082, 0.0068])),\n",
       " Row(id='9d2196265213dce8', target=0, topicDistribution=DenseVector([0.132, 0.7876, 0.0178, 0.0208, 0.0229, 0.0189])),\n",
       " Row(id='fb7a63a8e287b2d1', target=0, topicDistribution=DenseVector([0.4623, 0.0088, 0.0084, 0.0097, 0.0106, 0.5001]))]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"topicDistribution\", labelCol=\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = new_dataset.sampleBy(\"target\", fractions={0: 0.8, 1: 0.8}, seed=5757).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = new_dataset.join(train, on=\"id\", how=\"leftanti\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol=\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6444604194017056"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last time with CountVectorizer with 20k words in vocabulary we got 0.8275751487175559"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
